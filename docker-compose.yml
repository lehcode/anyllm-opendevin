version: "3.8"
services:
  # Devin Agent Service
  # Devin-Out-of-The-Box
  app:
    container_name: devin-app
    hostname: app-server
    build:
      dockerfile: docker/devin/app/Dockerfile
      args:
        workspace_dir: ${WORKSPACE_DIR:?}
        app_dir: ${APP_DIR:?}
        debug: ${DEBUG:-}
        cuda_version: ${CUDA_VERSION:?}
        cuda_image_os: ${CUDA_IMAGE_OS:?}
    env_file:
      - ./.env
      - ./docker/devin/app/.env
    command: entrypoint.sh
    depends_on:
      - backends
#      - proxy
    networks:
      opendevin-net:
        ipv4_address: 172.28.111.5
    volumes:
      - ./workspace:${WORKSPACE_DIR:?}
      - ./requirements.txt:/etc/opendevin/
      - ./Makefile:/etc/opendevin/
      - ./:${APP_DIR:?}/
      - devin_home_vol:/root
      - pip_cache:/root/.cache/pip
      - conda_cache:/root/anaconda/pkgs
#      - ./docker/devin/app/.condarc:/root/
      - conda_env:/root/miniconda3/envs/${VENV_NAME}
    working_dir: ${APP_DIR}
    tmpfs:
      - /var/devin:exec,size=1g
      - /run
      - /tmp
    ports:
      - "3000:3000"
      - "${JUPYTER_PORT:?}:${JUPYTER_PORT:?}"
    tty: true
    stdin_open: true
    restart: no
    deploy:
      resources:
        reservations:
          devices:
            - driver: "nvidia"
              count: 1
              capabilities: [ "gpu" ]

  # LightLLM proxy + MemGPT üè¨
  backends:
    container_name: devin-backends
    hostname: devin-backends
    build:
      dockerfile: docker/backends/Dockerfile
      args:
        litellm_port: ${LITELLM_PORT:?}
        jupyter_port: ${JUPYTER_PORT:?}
        debug: ${DEBUG:?}
        cuda_version: ${CUDA_VERSION:?}
        cuda_image_os: ${CUDA_IMAGE_OS:?}
        venv_name: ${VENV_NAME:?}
        memgpt_config_path: ${MEMGPT_CONFIG_PATH:-/root/.memgpt/config}
    env_file:
      - .env
      - docker/backends/.env
    working_dir: /root
    volumes:
      - ${HOST_MODELS_DIR:?}:/root/.ollama/models
      - pip_cache:/root/.cache/pip
      - conda_cache:/root/anaconda/pkgs
      - conda_env:/root/miniconda3/envs/${VENV_NAME:?}
#      - ./docker/backends/.condarc:/root/
      - ./docker/backends/memgpt_config:/root/.memgpt/config
      - ./docker/env_debug.sh:/usr/bin/env_debug
    #      - ./.mitmproxy/mitmproxy-ca.pem:/usr/local/share/ca-certificates/mitmproxy-ca.crt
    tmpfs:
      - /run
      - /tmp
    depends_on:
      - ollama
      - litellm
#      - proxy
    networks:
      - opendevin-net
    tty: true
    stdin_open: true
    entrypoint: /opt/nvidia/nvidia_entrypoint.sh
    healthcheck:
      test: ["CMD", "litellm", "--health"]
      interval: 1m
      timeout: 10s
      retries: 3
      start_period: 2m
    deploy:
      resources:
        reservations:
          devices:
            - driver: "nvidia"
              count: 1
              capabilities: [ "gpu" ]

  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: litellm-backend
    command: "--config /etc/config.yaml --port ${LITELLM_PORT} --num_workers 8 --detailed_debug"
    environment:
#      LITELLM_DEFAULT_MODEL: ${LITELLM_DEFAULT_MODEL:?}
      DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:${POSTGRES_CONTAINER_PORT}/${POSTGRES_DB}
    ports:
      - "${LITELLM_PORT}:${LITELLM_PORT}"
    volumes:
      - ./docker/litellm/config.yaml:/etc/config.yaml
      - pip_cache:/root/.cache/pip
    depends_on:
      - redis
      - postgres
      - ollama
    networks:
      - opendevin-net

  redis:
    image: redis/redis-stack:latest
    container_name: redis-stack-server
    env_file:
      - .env
      - docker/redis/redis.env
    ports:
      - "${REDIS_SERVER_HOST_PORT}:6379"
      - "${REDIS_INSIGHT_HOST_PORT}:8001"
    volumes:
      - redis_data:${REDIS_DATA:-/data}
    networks:
      - opendevin-net

  postgres:
    image: postgres:latest
    container_name: db-postgres
    env_file:
      - .env
      - docker/postgres/postgres.env
    volumes:
      - posgtres_data:${POSTGRES_DATA:-/var/lib/postgres}
    ports:
      - "${POSTGRES_HOST_PORT}:${POSTGRES_CONTAINER_PORT}"
    tmpfs:
      - /var/run:size=1m
    networks:
      - opendevin-net

  # Ollama service
  ollama:
    image: ollama/ollama
    container_name: ollama-service
    hostname: ollama
    platform: linux/amd64
    env_file:
      - .env
      - docker/ollama/ollama.env
    tty: false
    ports:
      - "22434:11434"
    volumes:
      - ${HOST_MODELS_DIR:?}:/root/.ollama/models
      - pip_cache:/root/.cache/pip
    deploy:
      resources:
        reservations:
          devices:
            - driver: "nvidia"
              count: 1
              capabilities: [ "gpu" ]
    networks:
      - opendevin-net

#  rev-nginx:
#    image: nginx:latest
#    volumes:
#      - ./docker/nginx/nginx.conf:/etc/nginx/nginx.conf
##    ports:
##      - "3000:3000"
#    networks:
#      - opendevin-net
#    depends_on:
#      - devin-ui
#      - devin-app

#  proxy:
#    build:
#      dockerfile: ./docker/mitmproxy/Dockerfile
#      args:
#        version: ${MITMPROXY_VERSION:?}
#        proxy_wheel: mitmproxy-${MITMPROXY_VERSION:?}-py3-none-any.whl
#        mitmproxy_dir: ${MITMPROXY_DIR:?}
#        uid: 1000
#        gid: 1000
#    container_name: mitmproxy
#    command: "mitmweb --web-host 0.0.0.0 --web-port 8081 --mode reverse:http://ollama:11434@44444 --verbose --anticache --anticomp"
#    networks:
#      - opendevin-net
#    volumes:
#      - ./.mitmproxy:${MITMPROXY_DIR:?}
#    tty: true
#    stdin_open: true
#    ports:
#      - "0.0.0.0:8081:8081"
#    depends_on:
#      - ollama


  # UI service
  ui:
    container_name: devin-ui
    build:
      dockerfile: docker/devin/ui/Dockerfile
      args:
        ui_dir: ${UI_DIR:-/var/www/od_ui}
        node_version: ${NODE_VERSION:?}
        node_env: ${NODE_ENV:-production}
        debug: ${DEBUG:?}
    env_file: docker/ui/.env
    environment:
      BACKEND_HOST: 172.28.111.5:3000
      FRONTEND_PORT: 3001
    networks:
      - opendevin-net
    ports:
      - "18888:18888"
      - "3001:3001"
    depends_on:
      - app
    tty: true
    working_dir: ${UI_DIR:-/var/www/od_ui}

networks:
  opendevin-net:
    ipam:
      driver: default
      config:
        - subnet: 172.28.0.0/16
          gateway: 172.28.111.1

volumes:
  devin_home_vol:
  posgtres_data:
  redis_data:
  pip_cache:
  conda_cache:
  conda_env:
