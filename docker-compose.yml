version: "3.8"
services:
  # Devin Out-of-The-Box Agent Service
  app:
    container_name: devin-app
    hostname: app-server
    image: oppendevin-cuda_11.3.1-conda:latest
    build:
      dockerfile: docker/devin/app/Dockerfile
      args:
        workspace_dir: ${WORKSPACE_DIR:?}
        app_dir: ${APP_DIR:?}
        debug: ${DEBUG:-}
        venv_name: od_env
        timezone: ${TIMEZONE:?}
    env_file:
      - ./.env
      - ./docker/devin/app/.env
    environment:
      CONDA_ROOT: ${CONDA_DIR:?}
    command: /entrypoint.sh
#    depends_on:
#      - memgpt
#      - proxy
    networks:
      opendevin-net:
        ipv4_address: 172.28.111.5
    volumes:
      - ./workspace:${WORKSPACE_DIR:?}
      - ./requirements.txt:${APP_DIR:?}/
      - ./Makefile:${APP_DIR:?}/
      - ./:${APP_DIR:?}/
      - devin_home_vol:/root
      - pip_cache_vol:${PIP_CACHE_DIR:?}
      - conda_cache_vol:${CONDA_CACHE_DIR:?}
      - ./docker/devin/app/.condarc:${CONDA_DIR}/.condarc
      - conda_env_vol:/root/miniconda3/envs/${VENV_NAME:?}
      - ./docker/env_debug.sh:/usr/local/bin/env_debug
    working_dir: ${APP_DIR}
    tmpfs:
      - /run
      - /tmp
      - /var/tmp
      - /var/run
    ports:
      - "3000:3000"
      - "${JUPYTER_PORT:?}:${JUPYTER_PORT:?}"
    tty: true
    stdin_open: true
    restart: no
    deploy:
      resources:
        reservations:
          devices:
            - driver: "nvidia"
              count: 1
              capabilities: [ "gpu" ]

  # MemGPT üè¨
  memgpt:
    container_name: devin-memgpt
    hostname: devin-memgpt
    image: oppendevin_memgpt-cuda_11.3.1-conda:latest
    build:
      dockerfile: docker/memgpt/Dockerfile
      args:
#        litellm_port: ${LITELLM_PORT:?}
#        jupyter_port: ${JUPYTER_PORT:?}
        debug: ${DEBUG:?}
        venv_name: ${VENV_NAME:?}
        memgpt_config: /etc/memgpt/config.json
        timezone: ${TIMEZONE:?}
        conda_cache_dir: ${CONDA_CACHE_DIR:?}
        pip_cache_dir: ${PIP_CACHE_DIR:?}
        ollama_openai_port: ${LITELLM_PORT:?}
    env_file:
      - .env
      - docker/memgpt/.env
    environment:
      CONDA_ROOT: ${CONDA_DIR:?}
      CONDA_PREFIX: ${CONDA_DIR:?}
      MEMGPT_CONFIG_PATH: ${MEMGPT_CONFIG_FILE:?}
      PGVECTOR_TEST_DB_URL: ${PGVECTOR_TEST_DB_URL:?}
    working_dir: /root
    volumes:
      - ${HOST_MODELS_DIR:?}:/root/.ollama/models
      - pip_cache_vol:${PIP_CACHE_DIR:?}
      - conda_cache_vol:${CONDA_CACHE_DIR}
      - conda_env_vol:/root/miniconda3/envs/${VENV_NAME:?}
      - ./docker/memgpt/.condarc:${CONDA_DIR}/.condarc
      - ./docker/memgpt/memgpt_config:${MEMGPT_CONFIG_FILE:?}
    #      - ./.mitmproxy/mitmproxy-ca.pem:/usr/local/share/ca-certificates/mitmproxy-ca.crt
    tmpfs:
      - /tmp
      - /var/tmp
    depends_on:
      - ollama
      - litellm
##      - proxy
    networks:
      - opendevin-net
    tty: true
    stdin_open: true
    entrypoint: /opt/nvidia/nvidia_entrypoint.sh
    deploy:
      resources:
        reservations:
          devices:
            - driver: "nvidia"
              count: 1
              capabilities: [ "gpu" ]

  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: litellm-backend
    command: "--config /etc/config.yaml --port ${LITELLM_PORT} --num_workers 8 --detailed_debug"
    environment:
#      LITELLM_DEFAULT_MODEL: ${LITELLM_DEFAULT_MODEL:?}
      DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:${POSTGRES_CONTAINER_PORT}/${POSTGRES_DB}
    ports:
      - "${LITELLM_PORT}:${LITELLM_PORT}"
    volumes:
      - ./docker/litellm/config.yaml:/etc/config.yaml
      - pip_cache_vol:${PIP_CACHE_DIR:?}
      - ./docker/env_debug.sh:/usr/local/bin/env_debug
    depends_on:
      - redis
      - postgres
      - ollama
    networks:
      - opendevin-net
    healthcheck:
      test: [ "CMD", "litellm", "--health" ]
      interval: 1m
      timeout: 10s
      retries: 3
      start_period: 2m
    tmpfs:
      - /run
      - /var/run
      - /tmp
      - /var/tmp

  redis:
    image: redis/redis-stack:latest
    container_name: redis-stack-server
    env_file:
      - .env
      - docker/redis/redis.env
    ports:
      - "${HOST_REDIS_SERVER_PORT}:6379"
      - "${HOST_REDIS_INSIGHT_PORT}:8001"
    volumes:
      - redis_data:${REDIS_DATA:-/data}
    networks:
      - opendevin-net

  postgres:
    image: postgres:latest
    container_name: db-postgres
    env_file:
      - .env
      - docker/postgres/postgres.env
    volumes:
      - posgtres_data:${POSTGRES_DATA:-/var/lib/postgres}
    ports:
      - "${POSTGRES_HOST_PORT}:${POSTGRES_CONTAINER_PORT}"
    tmpfs:
      - /var/run:size=1m
      - /tmp
    networks:
      - opendevin-net

  # Ollama service
  ollama:
    image: ollama/ollama
    container_name: ollama-service
    hostname: ollama
    platform: linux/amd64
    env_file:
      - .env
      - docker/ollama/ollama.env
    tty: false
    ports:
      - "22434:11434"
    volumes:
      - ${HOST_MODELS_DIR:?}:/root/.ollama/models
      - pip_cache_vol:${PIP_CACHE_DIR:?}
    deploy:
      resources:
        reservations:
          devices:
            - driver: "nvidia"
              count: 1
              capabilities: [ "gpu" ]

    networks:
      - opendevin-net
    tmpfs:
      - /run
      - /var/run
      - /tmp
      - /var/tmp

  mitm:
    build:
      dockerfile: ./docker/mitmproxy/Dockerfile
      args:
        version: ${MITMPROXY_VERSION:?}
        proxy_wheel: mitmproxy-${MITMPROXY_VERSION:?}-py3-none-any.whl
        mitmproxy_dir: ${MITMPROXY_DIR:?}
        uid: 1000
        gid: 1000
    container_name: mitmproxy
    command: "mitmweb --web-host 0.0.0.0 --web-port 8081 --mode reverse:http://ollama:11434@44444 --verbose --anticache --anticomp"
    networks:
      - opendevin-net
    volumes:
      - ./.mitmproxy:${MITMPROXY_DIR:?}
    tty: true
    stdin_open: true
    ports:
      - "0.0.0.0:8081:8081"
    depends_on:
      - ollama
      - memgpt
      - app
    tmpfs:
      - /run
      - /var/run
      - /tmp
      - /var/tmp

  # UI service
  ui:
    container_name: devin-ui
    build:
      dockerfile: docker/devin/ui/nginx/Dockerfile
      args:
        ui_dir: ${UI_DIR:-/var/www/od_ui}
        node_version: ${NODE_VERSION:?}
        node_env: ${NODE_ENV:-production}
        debug: ${DEBUG:?}
    env_file: docker/devin/ui/.env
    environment:
      BACKEND_HOST: 172.28.111.5:3000
      FRONTEND_PORT: 3001
    networks:
      - opendevin-net
    ports:
      - "18888:18888"
      - "8088:8088"
      - "3001:3001"
      - "8443:8443"
    depends_on:
      - app
    tty: true
    working_dir: ${UI_DIR:-/var/www/od_ui}
    volumes:
      - ./docker/devin/ui/nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./frontend/dist/:/var/www/app/dist/

networks:
  opendevin-net:
    ipam:
      driver: default
      config:
        - subnet: 172.28.0.0/16
          gateway: 172.28.111.1

volumes:
  devin_home_vol:
  posgtres_data:
  redis_data:
  pip_cache_vol:
  conda_cache_vol:
  conda_env_vol:
